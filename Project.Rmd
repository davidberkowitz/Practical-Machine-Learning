---
title: "Project - Practical Machine Learning"
author: "David Berkowitz"
date: "March 3, 2015"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

```{r}
library (caret, quietly=TRUE)
library (randomForest, quietly=TRUE)
set.seed (123)
```

### Read in the data

```{r, echo=FALSE}
if (file.exists ("training.RDS"))
  {
  # efficient RDS exists, read it
  print ("RDS exists, reading it")
  data.raw = readRDS ("training.RDS")
  print (".")
  } else
  {
  # read the CSV
  print ("RDS does NOT exist, reading the CSV")  
  
  data.raw = read.csv ("pml-training.csv")
  print (".")
  
  # create the efficient RDS
  print ("creating the RDS")
  saveRDS (data.raw, "training.RDS")
  print (".")
  }  
```

### Feature selection (reduce the number of columns)

Many of the columns are statistical calculations (min_, max_, avg_, stddev_, var_, skewness_, kurtosis_) on the raw data measurements. I chose to build my classifier only on raw data columns: gyro_, accel_ and magnet_. 

```{r}
names = names (data.raw) # get the list of column names

gyros = grep ("^gyros", names)
accel = grep ("^accel", names)
magnet = grep ("^magnet", names)
class = grep ("^classe", names) # add the activity column
user = grep ("^user", names) # add the user name column

data = data.raw [, c(gyros, accel, magnet, class, user)] # only include wanted columns
data = na.omit (data) # omit NA values which are not appreciated by later functions
```

### Partition the data

```{r}
set.seed (12345)
dp = createDataPartition (y = data$classe, p = 0.6, list=FALSE)
myTraining = data [dp,] # training set has random 60%
myTesting = data [-dp, ] # testing set has the remaining 40%

a = nrow (data) ; b = nrow (myTraining) ; c = nrow (myTesting)
check = a - b - c
cbind (a, b, c, check) # check should equal 0
```

### Create the classifier based on my training set

```{r}
rf = randomForest (classe ~ ., data=myTraining); rf
```

### Evaluate the classifier based on my testing set

```{r}
predictions = predict (rf, myTesting)
confusionMatrix (predictions, myTesting$classe)
```

This simple predictor does quite well with 98.38% accuracy, which is in line with the OOB estimated error rate of 1.55%.

### Predict based on the real testing set

```{r}
testingData = read.csv ("pml-testing.csv")
testingData = testingData [, c(gyros, accel, magnet, class, user)] # same dataset filtering as before
testingData = na.omit (testingData)

predictions = predict (rf, testingData); predictions
```

```{r, echo=FALSE}
# provided function to write individual files for each row
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files (predictions)
```

This "simple" classifier was sufficient to correctly predict all 20 values for the course project submission.

### Cross Validation

```{r}
table (data$user_name, data$classe)
#ggplot (data, aes (x=classe)) + geom_bar() + facet_wrap (~ user_name)
```

There appear to be sufficient data points for each activity for each user. I will cross validate by user_name.

```{r}
crossValidate = function (name)
  {
  testingRowNumbers = training = testing = NULL
  a = b = c = check = NULL
  rf = cm = NULL
  
  testingRowNumbers = grep (name, data$user_name)
  training = data [-testingRowNumbers,]
  testing = data [testingRowNumbers,]
  
  a = nrow (data) ; b = nrow (training) ; c = nrow (testing); check = a - b - c
  cbind (a, b, c, check) # check should equal 0
  
  rf = randomForest (classe ~ ., data=training)
  predictions = predict (rf, testing)
  cm = confusionMatrix (predictions, testing$classe)
  cat (name, "accuracy= ", cm$overall ["Accuracy"], "\n")
  return (rf);
  }
```

```{r}
rf = crossValidate ("adelmo"); rf
rf = crossValidate ("carlitos"); rf
rf = crossValidate ("charles"); rf
rf = crossValidate ("eurico"); rf
rf = crossValidate ("jeremy"); rf
rf = crossValidate ("pedro"); rf
```

```{r, echo=FALSE}
names = c ("adelmo", "carlitos", "charles", "eurico", "jeremy", "pedro")
oobErrorRates = c (0.83, 0.79, 0.75, 0.90, 0.86, 0.81)
accuracy = c (0.283, 0.578, 0.562, 0.177, 0.568, 0.194)
```

### Results

```{r}
cbind (names, oobErrorRates, accuracy)

mean (oobErrorRates) # average expected error rate (percentage)
mean (accuracy) # average measured accuracy
```

# Further Investigations

After the later video lectures I wanted to start over and apply some of the techniques discussed.
Here are the results.

### Read in the data set

```{r}
setwd ("/Users/davidberkowitz/Projects/coursera/Practical Machine Learning")

data = read.csv ("pml-training.csv", 
                        header=TRUE, 
                        as.is = TRUE,
                        stringsAsFactors = FALSE, 
                        #sep=',',
                        )
dim (data)
summary (data)
```

There are 160 columns in the raw data set.

### Remove columns with NAs

```{r}
NAindex = apply (data, 2, function(x) {sum(is.na(x))})
NAindex
data = data [,which (NAindex == 0)]

dim (data)
summary (data)
```

Number of columns reduced from 160 to 93.

### Preprocess the numeric columns. Preserve the outcome column.

```{r}
c = data$classe # save

v = which (lapply (data, class) %in% "numeric") # numerica columns only
v

ppo = preProcess (data [,v], method=c ('knnImpute', 'center', 'scale'))
data <- predict (ppo, data [,v])

data$classe = as.factor (c) # restore

dim (data)
summary (data)
str (data)
```

### Remove columns with near zero variance

```{r}
nzv <- nearZeroVar (data, saveMetrics=TRUE)
nzv
data = data [,nzv$nzv==FALSE]

dim (data)
summary (data)
str (data)
```

Number of columns reduced from 93 to 28. Pretty good compared to the original 160.

### Partition the data into training and test sets. 

Note that all preprocessing/cleaning of the data is performed **before** the partitioning.

```{r}
set.seed (12345)

dp = createDataPartition (y = data$classe, p = 0.6, list=FALSE)
myTraining = data [dp,] # training set has random 60%
myTesting = data [-dp, ] # testing set has the remaining 40%

dim (myTraining)
dim (myTesting)
```

### Train the model - fit

```{r}
fit = randomForest (classe ~ ., data=myTraining)
fit
plot (fit)

imp = varImp (fit)
imp
```

Expected error rate of 0.8% sounds pretty good to me :-)

### Train the model in a slightly different way - fit2

Note this method takes a **loooong** time.

```{r}
fit2 = train (classe ~.,
              method="rf",
              data=myTraining,
              trControl=trainControl (method='cv'), number=5, allowParallel=TRUE )
fit2

imp = varImp (fit2)
imp
```

Note that this model was cross validated 10 fold.

### Check against the test set - both fit and fit2

```{r}
predictions = predict (fit, myTesting) #fit
confusionMatrix (predictions, myTesting$classe)

predictions = predict (fit2, myTesting) #fit2
confusionMatrix (predictions, myTesting$classe)
```

### Conclusion

The accuracy of the second model, which takes much longer to calculate, is **not** more accurate than the first model. In either case, I am very happy with a 99% accuracy.